{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a72c20",
   "metadata": {},
   "source": [
    "# web scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c344d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b703a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_real_estate_data(url,rent=None):\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    print(response.status_code)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract property listings\n",
    "        listings = soup.find_all('div', class_='lazyload-wrapper')\n",
    "        \n",
    "        # Extract data from each listing\n",
    "        for listing in listings:\n",
    "            # Extract property details (e.g., price, location, etc.)\n",
    "            link_tag = listing.find('div', class_='results_card')\n",
    "            link = link_tag.find('a')['href']\n",
    "            \n",
    "            price = listing.find('span', class_='price').text\n",
    "            if rent:\n",
    "                _ ,price,_, _ = price.replace(\"\\xa0\",\" \").split(\" \")\n",
    "            else:\n",
    "                try:\n",
    "                    _ ,price = price.strip().replace(\"\\xa0\",\" \").strip().split(\" \")\n",
    "                    \n",
    "                except:\n",
    "                    price = \"0,\"\n",
    "                \n",
    "            title_location_div = listing.find('div', class_='card_title')\n",
    "            title=title_location_div.find(\"h2\").text.strip()\n",
    "            location=title_location_div.find(\"h5\").text.strip()\n",
    "            \n",
    "            # Extract image URL if available\n",
    "            image_url_div = listing.find('div', class_='results_card_media')\n",
    "            image_url = image_url_div.find_all('img')[2]['src'] \n",
    "\n",
    "            # desciptions\n",
    "            desc_tag = listing.find('ul',class_='results_card_details')            \n",
    "            desc = \"\"\n",
    "            \n",
    "            # Extract the text of the <li> tag and concatenate it with the existing descriptions\n",
    "            for li_tag in desc_tag.find_all( ['li','p'] ):\n",
    "                desc +=  li_tag.text + \" | \"\n",
    "                \n",
    "            response_click=requests.get(link)\n",
    "            if response_click.status_code == 200:\n",
    "                soup_click = BeautifulSoup(response_click.text,\"html.parser\")\n",
    "                sub_listings = soup_click.find(\"div\",class_=\"tower_description\")\n",
    "                description = sub_listings.find(\"p\").text.strip() if sub_listings.find(\"p\") else \"\"\n",
    "            else:\n",
    "                description=\"\"\n",
    "\n",
    "                \n",
    "            # get agents contact\n",
    "            \n",
    "            ag_name = listing.find('div',class_='agent_name').find('span').text\n",
    "            \n",
    "            ag_whatsapp = listing.find('div',class_='agent_contact').find('a')['href']\n",
    "            ag_call = listing.find('div',class_='agent_contact').find_all('a')[1]['href']\n",
    "            \n",
    "            #get the bed and = bath\n",
    "            \n",
    "            bed_bath_div = listing.find(\"div\",class_=\"about_tour\")\n",
    "            bed , bath , sqft = bed_bath_div.find_all(\"span\")  # this will be 3 list 0-bed 1-bath 2-hight(not using it)\n",
    "            sqft, _ = sqft.text.split(\" \")\n",
    "            number_of_beds =int(bed.text) if  bed  else 0\n",
    "            number_of_baths = int(bath.text) if  bath.text.isdigit()  else 0\n",
    "            size_in_sqft = int(sqft.replace(\",\",\"\")) if sqft else 0\n",
    "            \n",
    "            \n",
    "            \n",
    "            properties= {\n",
    "                'title': title,\n",
    "                'description': desc,\n",
    "                'price_in_aed': int(price.replace(\",\",\"\")), \n",
    "                'location': location,\n",
    "                'number_of_beds': number_of_beds,\n",
    "                'number_of_baths':number_of_baths ,\n",
    "                \"size_in_sqft\": size_in_sqft,\n",
    "                'sale_type': \"rent\" if rent else \"buy\",\n",
    "                'page_url':link,\n",
    "                'image_url': image_url,\n",
    "                'agent_name':ag_name,\n",
    "                'whatsapp_link':ag_whatsapp,\n",
    "                'call_agent':ag_call,\n",
    "                'long_description': description\n",
    "            }\n",
    "\n",
    "        return properties\n",
    "    else:\n",
    "        print(\"Failed to fetch the page\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c64f7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# ## NOW LETS GRAP SOME MORE PAGES AND STORE TO DF\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "num_pages = 10\n",
    "\n",
    "starter = 0\n",
    "for _ in range(50):\n",
    "    \n",
    "    rent=True\n",
    "    rent_listings= []\n",
    "    for i in range(num_pages):\n",
    "        url_rent= f\"https://www.allsoppandallsopp.com/dubai/properties/residential/lettings/page-{starter+i+1}\"\n",
    "        listings = scrape_real_estate_data(url_rent,rent)\n",
    "        rent_listings.append(listings)\n",
    "#         if (i + 1) % 4 == 0:  # Add a wait every 4 iterations\n",
    "#             time.sleep(10)  # Adjust the sleep duration\n",
    "        time.sleep(300)\n",
    "        print(\"rent_page : \", f\"{starter+i+1}/500\")\n",
    "    rent_df = pd.DataFrame(rent_listings)\n",
    "    rent_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    time.sleep(300)\n",
    "\n",
    "    sale_listings= []\n",
    "    for i in range(num_pages):\n",
    "        url_sale= f\"https://www.allsoppandallsopp.com/dubai/properties/residential/sales/page-{starter+i+1}\"\n",
    "        listings = scrape_real_estate_data(url_sale)\n",
    "        sale_listings.append(listings)\n",
    "#         if (i + 1) % 4 == 0:  # Add a wait every 4 iterations\n",
    "#             time.sleep(10)  # Adjust the sleep duration\n",
    "        time.sleep(300)\n",
    "        \n",
    "        print(\"sale_page : \", starter+i+1)\n",
    "\n",
    "        \n",
    "    sale_df = pd.DataFrame(sale_listings)\n",
    "    sale_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "\n",
    "    time.sleep(300)\n",
    "    starter = starter + 10\n",
    "\n",
    "\n",
    "rent_json=rent_listings\n",
    "sale_json=sale_listings\n",
    "\n",
    "# sale_df_0=sale_df\n",
    "# rent_df_0=rent_df\n",
    "\n",
    "\n",
    "rent_df.to_csv(\"2_real_estate_rents.csv\")\n",
    "sale_df.to_csv(\"2_real_estate_sales.csv\")\n",
    "\n",
    "sale_final = shuffle(pd.concat([rent_df,sale_df]))\n",
    "sale_final.to_csv(\"real_estate_sales.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77331de7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
